### 一、损失函数

#### 1、什么是损失函数

在模型训练阶段，用来衡量人工神经网络和人脑神经网络的相似度。一言以蔽之，损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。

![image-20240902093723010](D:\dev\php\magook\trunk\server\md\img\image-20240902093723010.png)



#### 2、常用的损失函数

##### 2.1、均方误差MSE

在**回归问题**中，均方误差损失函数用于度量样本点到回归曲线的距离，通过最小化平方损失使样本点可以更好地拟合回归曲线。均方误差损失函数（MSE）的值越小，表示预测模型描述的样本数据具有越好的精确度。

![image-20240830175621171](D:\dev\php\magook\trunk\server\md\img\image-20240830175621171.png)
$$
L(\hat{y}, y)=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}
$$


##### 2.2、交叉熵

它是被广泛使用的损失函数，无论是MLP还是基于自注意力机制的模型，比如Transformer。



**什么是信息量**

在信息论中，一种信号i出现的概率越高，当它发生时携带的**信息量**就越少，因此信息量和概率是一个反比例或倒数关系。



**什么是熵**

**熵**用来表示**一个系统（由一系列事件组成的集合）**的**不确定性**。熵越大则不确定性越大。为了与热力学中的熵区分，在信息论中，熵被称为香农熵。

在信息论中，**信息熵**描述一个信号源发出的信号携带的平均信息量（也就是信息量的期望），以此来描述一个系统的不确定性程度。



**不确定性**

**不确定性**指的是，你是否能从信息中辨别出当前系统处于哪一个状态。如果很容易辨别，则说明确定性强，则不确定性低。比如有两个信息：`今天太阳从西边升起`和`今天太阳从东边升起`。很显然前者的辨识度更高。

盲人摸象



**公式**
$$
信息量：I(x)=-\log{P(X_{i})}=\log{\frac{1}{P(X_{i})}} \\
信号量的期望：E(\log{\frac{1}{P(X_{i})}}) ~正好就是信息熵 \\
信息熵：H(X)=-\sum_{i}P(X_{i})\log_{b}P(X_{i})
=\sum_{i}P(X_{i})\log_{b}\frac{1}{P(X_{i})} \\
$$
特性：

- P(x)是一种概率分布，表示系统处于某一种状态下的概率，取值[0,1]，总和为1；

- 其中的 b 代表底数，即单位，比如 2, 10, e 等等，常用的就是 2，这符合计算机中的二进制。

- 熵的最大值为 log{N}，即这是一个均匀分布的系统，每个事件发生的概率都是 1/N，比如掷骰子。



**示例**

![image-20240901141604795](D:\dev\php\magook\trunk\server\md\img\image-20240901141604795.png)

对于上面的这两个分布P(x)，我们来计算它们的熵。

info_entropy.py



**交叉熵**

用来评估当前训练得到的**概率分布**与真实分布的差异情况。 它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，不确定性越小，两个系统就越接近。
$$
H(Y,\hat{Y})=-\sum_{i=1}^{n}Y_{i}·\log{\hat{Y}_{i}}
=-\sum_{i=1}^{n}(y\log{\hat{y}}+(1-y)\log{(1-\hat{y}})) \\
$$


### 二、梯度下降法

#### 1、梯度

在某处的梯度可以简单理解为斜率，梯度的值就是求导，对于一元多次方程（曲线），梯度的值是导数；对于多元多次方程（曲面），梯度的值就是求偏导，然后再将具体的点坐标代入公式得到结果；于是一元多次方程在某点处的梯度就是一个值（斜率），而多元多次方程在某点处的梯度是一个向量。



假如有方程(ax+by+c+d)，其参数为(a, b, c, d)，我们将(x, y)看成常量（因为有样本数据），于是方程就变成了关于(a, b, c, d)的函数。
$$
\nabla a=\frac{\partial f}{\partial a} \\
\nabla b=\frac{\partial f}{\partial b} \\
\nabla c=\frac{\partial f}{\partial c} \\
\nabla d=\frac{\partial f}{\partial d} \\
\nabla f= [\nabla a, \nabla b, \nabla c, \nabla d]
$$


#### 2、梯度下降法



首先给参数(a, b, c, d)一个初始值，设置一个步长（学习率） η，然后根据梯度可以更新参数。
$$
a'=a-\eta · \nabla a \\
b'=b-\eta · \nabla b \\
c'=c-\eta · \nabla c \\
b'=d-\eta · \nabla d \\
$$


接下来设置目标结果，也就是收敛条件，常见的收敛条件包括：

- 损失函数值的变化量小于预设的阈值。
- 梯度的范数小于预设的阈值。
- 达到最大迭代次数。



![image-20240809174519960](D:\dev\php\magook\trunk\server\md\img\image-20240809174519960.png)



![image-20240809174436469](D:\dev\php\magook\trunk\server\md\img\image-20240809174436469.png)





#### 3、算法实现

如果损失函数是**均方误差MSE**，那么可以推导出梯度的计算公式：
$$
假设：y=ax+b 。\\
损失函数：f(\hat{y},y) = \sum(\hat{y}-y)^2 \\
那么：\nabla a=\frac{\partial f}{\partial a}= \frac{\partial (\sum(\hat{y}-y)^2)}{\partial a} \\
=2·(\hat{y}-y)·\frac{\partial (\sum(\hat{y}-y))}{\partial a} \\
=2·(\hat{y}-y)·\frac{\partial y}{\partial a} \\
=2·(\hat{y}-y)·x \\
然后：\nabla b=2·(\hat{y}-y) \\
所以，综合来看，\theta_{n}=\theta_{n-1}-\eta·(\hat{y}-y)
$$
代码示例

gradient_descent_2.py

gradient_descent_3.py

gradient_descent_1.py



#### 4、随机梯度下降法SGD

每一次迭代随机选择几个少量的样本进行梯度计算。

行进路线就像醉汉，比较波折。好处是提升了计算速度（每次只需要计算少量样本点），但是却牺牲了精准度，但是大方向是对的。



#### 5、梯度反向传播BackPropagation

前面讲的使用梯度下降法求解损失函数的最小值是个原理，但是具体到多层神经网络这个场景中，还无法解决问题，因为这个损失函数是一个总的概括，如何求出每一层中的w梯度和b梯度呢，这就要用到反向传播算法逐一算出各个参数的梯度。反向传播的基础是微积分中的导数**链式法则**，我们可以从输出层开始，逐层向前计算每个参数的梯度。

![image-20240902093723010](D:\dev\php\magook\trunk\server\md\img\image-20240902093723010.png)

我们知道在多层神经网络中，各层的函数表示为：
$$
L1=(w_{11}·x_{1}+b_{11}) + (w_{21}·x_{2}+b_{21}) \\
L2=(w_{12}·x_{1}+b_{12}) + (w_{22}·x_{2}+b_{22}) \\
.... \\
L8=w_{18}·\sigma_{1}(L1)+b_{18}+w_{28}·\sigma_{2}(L2)+b_{28} \\
y=\sigma_{11}(L11)
$$
**链式法则：**
$$
假设：y=g(x)，z=f(y)，那么z关于x的函数可以写成：z=f(g(x))，这恰恰是多层神经网络的表示。 \\ 
为了计算方便，设~z=h(x)。\\
我们知道 \frac{dy}{dx}=g'(x)，\frac{dz}{dy}=f'(y)，那么z对x的导数\frac{dz}{dx}=\frac{dz}{dy}·\frac{dy}{dx} \\
当变量是多个的时候，偏导数依然适用。 \\
因此，我们就可以算出每个神经元的w梯度和b梯度，于是就可以更新参数 \\
w'_{t}=w_{t}-\eta·\nabla w_{t} \\
b'_{t}=b_{t}-\eta·\nabla b_{t}
$$




