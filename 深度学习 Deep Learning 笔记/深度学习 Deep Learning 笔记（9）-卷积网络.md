深度学习 Deep Learning 笔记（9）-卷积网络


卷积神经网络的原理：https://www.bilibili.com/video/BV1f54y1f7rs/?spm_id_from=333.788&vd_source=dd488f2825c3a352e192887d5d63e429


卷积网络（convolutional network）(LeCun, 1989)，也叫做 卷积神经网络（convolutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。卷积网络在诸多应用领域都表现优异。“卷积神经网络’’ 一词表明该网络使用了 卷积（convolution）这种数学运算。卷积是一种特殊的线性运算。卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。



#### 卷积运算Convolution

https://blog.csdn.net/raoxiaoya/article/details/131655348
$$
s(t)=(x*w)(t)=\int x(a)w(t-a)da
$$
x 函数称为输入，w 称为核函数。



卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互（sparse interactions）、 参数共享（parameter sharing）、 等变表示（equivariant representations）。另外，卷积提供了一种处理大小可变的输入的方法。我们下面依次介绍这些思想。

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而，卷积网络具有 稀疏交互（sparse interactions）（也叫做 **稀疏连接**（sparse connectivity）或者 稀疏权重（sparse weights））的特征。这是使核的大小远小于输入的大小来达到的。举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。这意味着我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统计效率。这也意味着为了得到输出我们只需要更少的计算量。这些效率上的提高往往是很显著的。如果有 *m* 个输入和 *n* 个输出，那么矩阵乘法需要 *m* *×* *n* 个参数并且相应算法的时间复杂度为 *O*(*m* *×* *n*)（对于每一个例子）。如果我们限制每一个输出拥有的连接数为 *k*，那么稀疏的连接方法只需要 *k* *×* *n* 个参数以及 *O*(*k* *×* *n*) 的运行时间。在很多实际应用中，只需保持 *k* 比 *m* 小几个数量级，就能在机器学习的任务中取得好的表现。稀疏连接的图形化解释如图 9.2 和图 9.3 所示。在深度卷积网络中，处在网络深层的单元可能与绝大部分输入是间接交互的，如图 9.4 所示。这允许网络可以通过只描述稀疏交互的基石来高效地描述多个变量的复杂交互。

![image-20240830105414349](D:\dev\php\magook\trunk\server\md\img\image-20240830105414349.png)

![image-20240830105446331](D:\dev\php\magook\trunk\server\md\img\image-20240830105446331.png)

![image-20240830105456865](D:\dev\php\magook\trunk\server\md\img\image-20240830105456865.png)

![image-20240830105550408](D:\dev\php\magook\trunk\server\md\img\image-20240830105550408.png)



**参数共享**（parameter sharing）是指在一个模型的多个函数中使用相同的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。作为参数共享的同义词，我们可以说一个网络含有 绑定的权重（tied weights），因为用于一个输入的权重也会被绑定在其他的权重上。在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。这虽然没有改变前向传播的运行时间（仍然是 *O*(*k* *×* *n*)），但它显著地把模型的存储需求降低至 *k* 个参数，并且 *k* 通常要比 *m* 小很多个数量级。因为 *m* 和 *n* 通常有着大致相同的大小，*k* 在实际中相对于 *m* *×* *n* 是很小的。因此，卷积在存储需求和统计效率方面极大地优于稠密矩阵的乘法运算。图 9.5 演示了参数共享是如何实现的。



#### 池化Pooling

池化的本质就是抽取特征，对于输入的 Feature Map，选择某种方式对其进行**降维压缩**，目的是用一个小的矩阵来表示大的矩阵，并希望误差最小，这样做是为了加快后续的运算。

采用较多的一种池化过程叫**最大池化（Max Pooling）**，其具体操作过程如下：

![image-20240830114511218](D:\dev\php\magook\trunk\server\md\img\image-20240830114511218.png)

池化过程类似于卷积过程，如上图所示，表示的就是对一个 4×4 feature map邻域内的值，用一个 2×2 的filter，步长为2进行‘扫描’，选择最大值输出到下一层，这叫做 Max Pooling。

还有一种叫平均池化（Mean Pooling），顾名思义就是取平均值。

从原理看，池化层是不需要参数的，只需要选择哪种池化方法。



卷积网络中一个典型层包含三级（如图9.7 所示）。在第一级中，这一层并行地计算多个卷积产生一组线性激活响应。在第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为 探测级（detector stage）。在第三级中，我们使用 **池化函数**（pooling function）来进一步调整这一层的输出。

![image-20240830113320921](D:\dev\php\magook\trunk\server\md\img\image-20240830113320921.png)



池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。例如， 最大池化（max pooling）函数 (Zhou and Chellappa, 1988) 给出相邻矩形区域内的最大值。其他常用的池化函数包括相邻矩形区域内的平均值、*L* 2 范数以及基于据中心像素距离的加权平均函数。

不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似 不变（invariant）。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。图 9.8 用了一个例子来说明这是如何实现的。局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。但在一些其他领域，保存特征的具体位置却很重要。例如当我们想要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它们是否相交。



#### 数据类型

![image-20240830142405987](D:\dev\php\magook\trunk\server\md\img\image-20240830142405987.png)
