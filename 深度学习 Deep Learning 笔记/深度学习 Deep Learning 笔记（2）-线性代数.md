深度学习 Deep Learning 笔记（二）-线性代数

![image-20240624165819287](D:\dev\php\magook\trunk\server\md\img\image-20240624165819287.png)

高等数学符号及其含义

https://wenku.so.com/d/90cebd9b0ae0a3bf400b0be25cbe4a61





latexocr 命令



### 二、线性代数 Linear Algebra

28页开始

线性代数主要是面向连续数学，而非离散数学。



#### 2.1、标量，向量，矩阵，张量

标量：就是一个数字。

向量：一维数组，是一列。使用小写字母加粗的斜体表示，比如 ***b***

矩阵：二维数组。使用大写字母加粗斜体表示，比如 ***A***

张量：多维数组，它可以是任意维度，特殊的：0阶张量是标量，1阶张量是向量，2阶张量是矩阵。

矩阵转置：沿着左上角到右下角的对角线镜像。特殊的，只有一行元素的矩阵的T操作就是向量，有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其变为标准的列向量。标量可以看作是只有一个元素的矩阵。因此，标量的转置等于它本身。

矩阵的转置也可以理解为：将第一行变成第一列，第二行变成第二列，依次类推。
$$
(A^{\top})_{i,j}=A_{j,i}
$$
![image-20240624163529553](D:\dev\php\magook\trunk\server\md\img\image-20240624163529553.png)

#### 2.2、矩阵和向量的运算

矩阵的加减：矩阵的形状要一样，对应位置上的加减。

标量和矩阵运算：逐个元素运算。

矩阵和向量相加：要求向量的维度和矩阵列数相等，向量和矩阵的每一行相加，得到另一个矩阵。
$$
\left[{\begin{array}{r r r}{1}&{1}&{1}\\ {2}&{2}&{2}\\ {3}&{3}&{3}\end{array}}\right]
+
\left[{\begin{array}{r}{1}\\ {2}\\ {3}\end{array}}\right]
=
\left[{\begin{array}{r r r}{2}&{3}&{4}\\ {3}&{4}&{5}\\ {4}&{5}&{6}\end{array}}\right]
$$
矩阵乘以向量：***Ax*** = ***b*** ，矩阵的每一行与向量相乘，最后得到一个向量。
$$
\left[{\begin{array}{r r r}{1}&{1}&{1}\\ {2}&{2}&{2}\\ {3}&{3}&{3}\end{array}}\right]
\left[{\begin{array}{r}{1}\\ {2}\\ {3}\end{array}}\right]
=
\left[{\begin{array}{r}{6}\\ {12}\\ {18}\end{array}}\right]
$$


矩阵相乘：或者叉乘，记为 ***A × B***，或 ***AB***，要求***A***的列数等于***B***的行数，如果矩阵 ***A*** 的形状是 *m* *×* *n*，矩阵 ***B*** 的形状是 *n* *×* *p*，那么矩阵 ***C*** 的形状是 *m* *×* *p*。

矩阵相乘的特性：

- 矩阵相乘服从分配律，即 ***A**(**B** + **C**) = **AB** + **AC***。
- 也服从结合律，***A**(**BC**) = (**AB**)**C***。
- 但是不满足交换律，***AB** != **BA***。
- ***AB*** 的转置等于 ***B*** 的转置乘以 ***A*** 的转置。



矩阵点乘：记为 ***A · B***，各个对应元素相乘，要求矩阵的形状相等。用的少。



#### 2.3、单位矩阵和逆矩阵

单位矩阵：主对角线上的元素都是1，其他元素都是0，单位矩阵记为 ***I***。任何向量乘以单位矩阵都是向量本身。
$$
\left[{\begin{array}{r r r}{1}&{0}&{0}\\ {0}&{1}&{0}\\ {0}&{0}&{1}\end{array}}\right]
$$
矩阵的逆：满足公式 `A的逆 乘以 A 等于一个单位矩阵`。
$$
A^{-1}A=I_{n}
$$

$$
由 Ax=b ，可以推导出， x = A^{-1}b
$$

对于方阵（m=n）来说，它的左逆和右逆是相等的。
$$
A^{-1}A=AA^{-1}
$$


#### 2.4、线性相关和生成子空间

33页 看不懂

针对方程 ***Ax=b***，假如***x***和***y***都是它的解，那么 ***z*** = α***x*** + (1 *−* *α*)***y*** 也是它的解，其中 α 取任意实数。扩展一下，针对任意方程，该结论也成立。

![image-20240628162305849](D:\dev\php\magook\trunk\server\md\img\image-20240628162305849.png)

![image-20240628162502992](D:\dev\php\magook\trunk\server\md\img\image-20240628162502992.png)

![image-20240628162533504](D:\dev\php\magook\trunk\server\md\img\image-20240628162533504.png)

![image-20240628162549684](D:\dev\php\magook\trunk\server\md\img\image-20240628162549684.png)

**线性相关**

在线性代数里，矢量空间的一组元素中，若没有矢量可用有限个其他矢量的线性组合所表示，则称为线性无关或线性独立(linearly independent)，反之称为线性相关(linearly dependent)。例如在三维欧几里得空间R3的三个矢量(1, 0, 0)，(0, 1, 0)和(0, 0, 1)线性无关。但(2, −1, 1)，(1, 0, 1)和(3, −1, 2)线性相关，因为第三个是前两个的和。



#### 2.5、范数

向量的范数是一种用来刻画向量大小的一种度量。实数的绝对值，复数的模，三维空间向量的长度，都是抽象范数概念的原型。上述三个对象统一记为 x ，衡量它们大小的量记为 ‖x‖ （我们用单竖线表示绝对值，双竖线表示范数）

**向量的范数和模的区别：**向量的模是一个绝对值，指向量在二维或三维空间中的长度；范数是一个函数，它能表示向量在 N 维空间的长度。

![image-20240628163626981](D:\dev\php\magook\trunk\server\md\img\image-20240628163626981.png)

![image-20240628164726626](D:\dev\php\magook\trunk\server\md\img\image-20240628164726626.png)

![image-20240628164924133](D:\dev\php\magook\trunk\server\md\img\image-20240628164924133.png)



**向量的点乘**

```
x·y = x1*x2 + y1*y2 = ‖x‖‖y‖cosθ
```



#### 2.6、特殊类型的矩阵和向量

有些特殊类型的矩阵和向量是特别有用的。

![image-20240628172316579](D:\dev\php\magook\trunk\server\md\img\image-20240628172316579.png)

![image-20240628172857264](D:\dev\php\magook\trunk\server\md\img\image-20240628172857264.png)

![image-20240628173612327](D:\dev\php\magook\trunk\server\md\img\image-20240628173612327.png)

#### 2.7、特征分解

前提是方阵

`矩阵的分解就是将一个矩阵分解成几个矩阵的乘积。`

正如一个数字可以分解为其他多个数字相互运算的结果，一个矩阵可以分解成一组特征向量和特征值。

方阵 **A** 的 特征向量（eigenvector）是指与 **A** 相乘后相当于对该向量进行缩放的非零向量 **v**：
$$
Av = λv
$$
标量 *λ* 被称为这个特征向量对应的 特征值（eigenvalue）。
$$
A = Vdiag(λ)V^{-1}
$$

$$
A=QΛQ^{T}
$$



矩阵的特征分解给了我们很多关于矩阵的有用信息。

#### 2.8、奇异值分解

可以分解非方阵

（singular value decomposition, SVD），将矩阵分解为 奇异向量（singular vector）和 奇异值（singular value）。通过奇异值分解，我们会得到一些与特征分解相同类型的信息。然而，奇异值分解有更广泛的应用。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。例如，非方阵的矩阵没有特征分解，这时我们只能使用奇异值分解。

将矩阵 **A** 分解成三个矩阵的乘积：
$$
A = UDV^{T}
$$
假设 **A** 是一个 *m* *×* *n* 的矩阵，那么 **U** 是一个 *m* *×* *m* 的矩阵，**D** 是一个 *m* *×* *n*的矩阵，**V** 是一个 *n* *×* *n* 矩阵。

矩阵 **U** 和 **V** 都定义为正交矩阵，而矩阵 **D** 定义为对角矩阵。注意，矩阵 **D** 不一定是方阵。

SVD 最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。



#### 2.9、伪逆

![image-20240701100214217](D:\dev\php\magook\trunk\server\md\img\image-20240701100214217.png)



#### 2.10、迹运算

`迹运算返回的是矩阵主对角元素的和`
$$
\operatorname{Tr}(A)=\sum_{i}A_{i,i}
$$
![image-20240701102337798](D:\dev\php\magook\trunk\server\md\img\image-20240701102337798.png)



#### 2.11、行列式

行列式，记作 det(**A**)，是一个将方阵 **A** 映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积。如果行列式是 1，那么这个转换保持空间体积不变。

行列式的计算：

![image-20240701212248307](D:\dev\php\magook\trunk\server\md\img\image-20240701212248307.png)



#### 2.12、实例：主成分分析

主成分分析（principal components analysis, PCA）是一个简单的机器学习算法，可以通过基础的线性代数知识推导。

PCA 是一种线性`降维算法`，将高维空间的数据映射到低维。它只关心主成分而忽略掉次要的信息。

![image-20240701214602607](D:\dev\php\magook\trunk\server\md\img\image-20240701214602607.png)



比如说，将一个人拍扁，即将三维空间数据降到二维空间，那么从哪个方向拍能够最大限度的保留原始特征呢，这就是PCA要处理的问题。

![image-20240701215004420](D:\dev\php\magook\trunk\server\md\img\image-20240701215004420.png)



![image-20240702094527687](D:\dev\php\magook\trunk\server\md\img\image-20240702094527687.png)



这里说的不好理解，关于PCA更详细的解释：https://blog.csdn.net/u013719780/article/details/78352262



另外一个降维算法是`T-SNE`，他的核心思想是如果两个点在高维空间里相距较远，那么在低维空间中也应该相距较远，反之亦然。



scikit-learn 上关于PCA算法的说明和例子：https://scikit-learn.org/stable/modules/decomposition.html



**这一节的公式推导看不懂**



---

NumPy 是专门用来处理多维数组的。



如何求矩阵的特征值和特征向量？？



PCA要再看一下：https://blog.csdn.net/u013719780/article/details/78352262

