深度学习 Deep Learning 笔记（10）-序列建模：循环和递归网络



普通的神经网络只能单独的处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。

比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。



https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0



**循环神经网络**（recurrent neural network）或 RNN (Rumelhart *et al.*, 1986c)是一类用于处理序列数据的神经网络。就像卷积网络是专门用于处理网格化数据 **X**（如一个图像）的神经网络，循环神经网络是专门用于处理序列 **x** (1) *, . . . ,* **x** (*τ*) 的神经网络。正如卷积网络可以很容易地扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络可以扩展到更长的序列（比不基于序列的特化网络长得多）。大多数循环网络也能处理可变长度的序列。

![image-20240830150509957](D:\dev\php\magook\trunk\server\md\img\image-20240830150509957.png)



有记忆能力，可以理解一定的上下文，但是记忆力也就几步之内。

可以理解为一个神经网络NN，在不同的时刻`t`上有不同的NN，它们有不同的参数值，因此存储着不同的信息，但是`t`时刻的神经元可以从`t-1`时刻的神经元获取信息，这就是上下文。所谓的循环指的是时间上可以一层一层的从过去获得信息。

![image-20240814221559991](D:\dev\php\magook\trunk\server\md\img\image-20240814221559991.png)

![image-20240814221525926](D:\dev\php\magook\trunk\server\md\img\image-20240814221525926.png)





RNN可以理解上下文，可以用于NLP，语音，视频处理等，但是现在被自注意力模型transformer超越，但是RNN在小数据集，低算力的情况下依然可以非常有效。



**LSTM-RNN**

https://www.bilibili.com/video/BV1Z34y1k7mc/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0

为了解决RNN的记忆短的问题，人们发明了`长短期记忆网络LSTM（Long Short-term Memoery）`。

LSTM的秘诀就是增加了一个日记本存储长期记忆，并且有增加和删除日记的操作，这称为长期记忆。而短期记忆还是RNN来提供。

![image-20240814223903361](D:\dev\php\magook\trunk\server\md\img\image-20240814223903361.png)

粉色的三个C层就是长期记忆，也就是日记本，他与RNN中的隐藏层连接，互相通信，

![image-20240814224321273](D:\dev\php\magook\trunk\server\md\img\image-20240814224321273.png)



sigmoiid 函数取值(0, 1)，因此会抹掉一些不重要的信息，起到擦除作用，相当于选择性遗忘了部分记忆，相当于一个阀门，过滤重要特征，忽略无关信息。

tanh 函数取值(-1, 1)，相对于把这两天发生的事情进行梳理和归纳，也就是更新日记。



![image-20240814224608174](D:\dev\php\magook\trunk\server\md\img\image-20240814224608174.png)

Ct就是得到的新日记。