深度学习 Deep Learning 笔记（5）-机器学习基础



线性回归算法

拟合训练数据

VC维度（Vapnik-Chervonenkis dimension）

点估计

最大似然估计

频率派估计和贝叶斯估计

监督学习和无监督学习

随机梯度下降算法



#### 5.1、机器学习算法

机器学习算法是一种能够从数据中学习的算法。

大部分深度学习算法都是基于被称为随机梯度下降的算法求解的。

样本：每一个样本可能有多个特征，因此我们会将样本表示成一个向量。

机器学习可以完成的工作：分类，输入缺失分类，回归，转录，机器翻译，结构化输出，异常检测，合成和采样，缺失值填补，去噪，密度估计或概率质量函数估计



**Iris（鸢尾花卉）**数据集 (Fisher, 1936) 是统计学家和机器学习研究者使用了很久的数据集。它是 150 个鸢尾花卉植物不同部分测量结果的集合。每个单独的植物对应一个样本。每个样本的特征是该植物不同部分的测量结果：萼片长度、萼片宽度、花瓣长度和花瓣宽度。这个数据集也记录了每个植物属于什么品种，其中共有三个不同的品种。

**无监督学习算法**（unsupervised learning algorithm）训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。

**监督学习算法**（supervised learning algorithm）训练含有很多特征的数据集，不过数据集中的样本都有一个 标签（label）或 目标（target）。例如，Iris 数据集注明了每个鸢尾花卉样本属于什么品种。监督学习算法通过研究 Iris 数据集，学习如何根据测量结果将样本划分为三个不同品种。

**设计矩阵**：我们的数据集一般是拥有同样维度的多组样本，比如鸢尾花Iris 数据集包含 150 个样本，每个样本有四个维度的数据。这样，数据集可以表示为
$$
\mathbf{X}\in \mathbb{R}^{150\times4}
$$

##### 5.1.4、线性回归（linear regression）

我们的目标是建立一个系统，将 n 维 向量 X 作为输入，预测标量 y 作为输出。线性回归的输出是其输入的线性函数。
$$
{\hat{y}}=w^{T}x + b \\
其中 \space \hat{y} 表示模型预测 y 应该取的值，其中 w \in \mathbf{R}^{n} 是参数向量 \\
b 称为截距项
$$
我们可以将 **w** 看作是一组决定每个特征如何影响预测的 权重（weight）。



度量模型性能的一种方法是计算模型在测试集上的 均方误差（mean squarederror）。
$$
MSE = \frac{1}{m}\sum_{i}(\hat{y}-y)_{i}^{2}
$$

我们需要MSE最小，我们可以使它的导数为0，然后带入样本值就得到了w的值。公式如下
$$
w=\left(X^{\mathrm{(train)}\top}X^{\mathrm{(train)}}\right)^{-1}X^{\mathrm{(train)}\textsf{T}}y^{\mathrm{(train)}}
$$






#### 5.2、容量、过拟合、欠拟合

**欠拟合**是指模型不能在训练集上获得足够低的误差。

**过拟合**是指训练误差和和测试误差之间的差距太大。

![image-20240705175059921](D:\dev\php\magook\trunk\server\md\img\image-20240705175059921.png)

![image-20240705175120450](D:\dev\php\magook\trunk\server\md\img\image-20240705175120450.png)



**频率派估计和贝叶斯估计的区别**

对于随机变量，贝叶斯学派认为随机变量的分布也是个变量，频率学派则不这么认为。（因为分布是由参数决定的，你也可以认为，贝叶斯学派认为，参数也是个随机变量）。

频率学派和贝叶斯学派最大的差别其实产生于对参数空间的认知上。所谓参数空间，就是你关心的那个参数可能的取值范围。频率学派（其实就是当年的Fisher）并不关心参数空间的所有细节，他们相信数据都是在这个空间里的”某个“参数值下产生的（虽然你不知道那个值是啥），所以他们的方法论一开始就是从“哪个值最有可能是真实值”这个角度出发的。于是就有了最大似然（maximum likelihood）以及置信区间（confidence interval）这样的东西，你从名字就可以看出来他们关心的就是我有多大把握去圈出那个唯一的真实参数。而贝叶斯学派恰恰相反，他们关心参数空间里的每一个值，因为他们觉得我们又没有上帝视角，怎么可能知道哪个值是真的呢？所以参数空间里的每个值都有可能是真实模型使用的值，区别只是概率不同而已。于是他们才会引入先验分布（prior distribution）和后验分布（posterior distribution）这样的概念来设法找出参数空间上的每个值的概率。最好诠释这种差别的例子就是想象如果你的后验分布)是双峰的，频率学派的方法会去选这两个峰当中较高的那一个对应的值作为他们的最好猜测，而贝叶斯学派则会同时报告这两个值，并给出对应的概率。



**监督学习算法**

1、概率监督学习



2、支持向量机SVM（support vector machine）

https://www.zhihu.com/market/pub/120332118/manuscript/1543257253406523392

https://zhuanlan.zhihu.com/p/696741013

https://zhuanlan.zhihu.com/p/77750026

可以用来解决分类问题，使用一个直线将离散的点分割成两部分，在可选的分割直线中，希望中间的通道越宽越好，这样就说明分类效果好，同时又希望减小分类错误，所以就存在权衡问题，通道的两边接触到两边的部分点，这些点连起来构成了向量，于是这两个向量就起到了支撑作用，叫做支撑向量。

![image-20240807172608872](D:\dev\php\magook\trunk\server\md\img\image-20240807172608872.png)

![image-20240807180129605](D:\dev\php\magook\trunk\server\md\img\image-20240807180129605.png)

SVM 和神经网络的相似之处：

1. 都是监督学习模型：SVM 和神经网络都需要通过已有的数据来训练模型，然后再用这个模型来做预测或分类。

2. 目标都是分类或回归：这两者的主要任务都是将数据进行分类或回归分析。

3. 非线性处理：通过一些技巧（比如核技巧），SVM 也能处理非线性的数据，就像神经网络通过非线性激活函数处理复杂的数据一样。

SVM和神经网络的区别：

1. 结构不同：SVM 的结构相对简单，主要是寻找一个最佳超平面。而神经网络则是由多个层和大量节点组成，结构复杂得多。

2. 训练方式：SVM 的训练主要是通过解决一个[凸优化](https://www.zhihu.com/search?q=凸优化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3508785828})问题，这意味着找到的解是全局最优的。而神经网络通过[反向传播算法](https://www.zhihu.com/search?q=反向传播算法&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3508785828})进行训练，可能会陷入局部最优解。

3. 适用场景：SVM 在处理小规模数据集和高维数据时表现优秀，而神经网络在大数据和[复杂模式识别](https://www.zhihu.com/search?q=复杂模式识别&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3508785828})方面更具优势。



3、支持向量回归SVR（support vector regression）

尽管支持向量机通常用于分类，但它们可以同时用于回归和分类。SVR是一种用于回归分析的机器学习算法。

它与传统的线性回归方法不同，因为它在连续空间中找到最适合数据点的超平面，而不是将数据点拟合成一条直线。

![image-20240808111756927](D:\dev\php\magook\trunk\server\md\img\image-20240808111756927.png)

SVR相对于普通回归的优势包括：

- 非线性关系：SVR能够捕捉输入特征和目标变量之间的非线性关系。它通过使用核技巧来实现这一点，相比之下，线性回归假设输入特征和目标变量之间存在线性关系，而非线性回归则需要大量的计算。
- 对异常值的鲁棒性：与线性回归相比，SVR对异常值更具鲁棒性。SVR旨在最小化围绕预测值的一定间隔内的误差，这个间隔称为epsilon-insensitive区域。这种特性使得SVR对超出间隔之外的异常值影响较小，从而导致更稳定的预测结果。
- 对模型复杂度的控制：SVR通过正则化参数C和核参数等超参数提供对模型复杂度的控制。通过调整这些参数，可以权衡模型复杂度和泛化能力，而线性回归则不提供这种灵活性。
- 支持向量的稀疏性：SVR通常依赖于一个称为支持向量的训练实例子集来构建回归模型。这些支持向量对模型具有最重要的影响，并代表了用于确定决策边界的关键数据点。这种稀疏性特性使得SVR在内存使用和计算速度方面比线性回归更有效率，特别是对于大型数据集而言。此外，一个优点是在添加新的训练点后，如果它们位于间隔内，模型不会发生变化。

4、决策树

以类似枚举的方式来做分类，将数据以很小的单位拆分后挂在树上，一个节点就是一个类别。

![image-20240809103207376](D:\dev\php\magook\trunk\server\md\img\image-20240809103207376.png)



**无监督学习算法**

无监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。

有很多方式定义较简单的表示。最常见的三种包括低维表示、稀疏表示和独立表示。

主成分分析

k值聚类



**什么是损失函数（代价函数）**

https://zhuanlan.zhihu.com/p/261059231

用来衡量人工神经网络和人脑神经网络的相似度。一言以蔽之，损失函数（loss function）就是用来度量模型的预测值f(x)与真实值Y的差异程度的运算函数，它是一个非负实值函数，通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。

损失函数使用主要是在模型的训练阶段，每个批次的训练数据送入模型后，通过前向传播输出预测值，然后损失函数会计算出预测值和真实值之间的差异值，也就是损失值。得到损失值之后，模型通过反向传播去更新各个参数，来降低真实值与预测值之间的损失，使得模型生成的预测值往真实值方向靠拢，从而达到学习的目的。

在监督学习的过程中，其实就是希望机器建立的概率模型无限接近人为认定的概率，因此损失函数就是用来比较两个概率模型，如何比较两个概率模型（概率分布）？常用的方法有**最小二乘法，极大似然估计，交叉熵**。

人脑中对于某个事件（比如判断一张图片是否是猫）的概率模型是未知的，计算机只能通过样本训练来无限逼近这个模型。神经网络可以叠加很多个已知的基础概率模型，通过修改各个模型的参数来逼近任何一种现实中的概率模型，来实现人脑的判断。

**如何选择损失函数**：如果目标任务是回归，可以使用MSE；如果目标是分类，可以选择交叉熵。



**最小二乘法，均方误差损失函数MSE**
$$
L(\hat{y}, y)=\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2} \\
L(\hat{y}, y)=\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}
$$
在**回归问题**中，均方误差损失函数用于度量样本点到回归曲线的距离，通过最小化平方损失使样本点可以更好地拟合回归曲线。均方误差损失函数（MSE）的值越小，表示预测模型描述的样本数据具有越好的精确度。由于无参数、计算成本低和具有明确物理意义等优点，MSE已成为一种优秀的距离度量方法。尽管MSE在图像和语音处理方面表现较弱，但它仍是评价信号质量的标准，在回归问题中，MSE常被作为模型的经验损失或算法的性能指标。



**极大似然估计**
$$
L(\hat{y}, y)=max\sum_{i=1}^{n}(y_{i}·\log{\hat{y}_{i}} + (1-y_{i})·\log{(1-\hat{y}_{i})}) \\
=min-\sum_{i=1}^{n}(y_{i}·\log{\hat{y}_{i}} + (1-y_{i})·\log{(1-\hat{y}_{i})}) \\
$$


**交叉熵**

https://www.zhihu.com/question/27068465/answer/2517655344

**信息熵**用来表示**一个系统（由一系列事件组成的集合）**的**不确定性**。熵越大则不确定性越大。为了与热力学中的熵区分，在信息论中，熵被称为香农熵。

关于熵：https://zhuanlan.zhihu.com/p/611197924

在信息论中，一种信号i出现的概率越高，当它发生时携带的**信息量**就越少，因此信息量和概率是一个反比例或倒数关系，之所以加上log函数，是因为log函数有很好的数学表现，且它不改变函数的单调性。

在信息论中，**信息熵**描述一个信号源发出的信号携带的平均信息量（也就是信息量的期望），以此来描述一个系统的不确定性程度。所以，理解信息熵最好是从信息量的角度出发。

**不确定性**指的是，你是否能从信息中辨别出当前系统处于哪一个状态。如果很容易辨别，则说明确定性强，则不确定性低。比如有两个信息：`今天太阳从西边升起`和`今天太阳从东边升起`。很显然前者的辨识度更高。

比如掷骰子游戏中，骰子的每个面发生的概率都是六分之一，这样的系统，每一个事件都不具备特殊性，不具备辨别度，不确定性很高，熵很高。

**熵的公式**如下
$$
信息量：I(x)=-\log{P(X_{i})}=\log{\frac{1}{P(X_{i})}} \\
信号量的期望：E(\log{\frac{1}{P(X_{i})}}) ~正好就是信息熵 \\
信息熵：H(X)=-\sum_{i}P(X_{i})\log_{b}P(X_{i})
=\sum_{i}P(X_{i})\log_{b}\frac{1}{P(X_{i})} \\
P(x)是一种概率分布，表示系统处于某一种状态下的概率，取值[0,1]，总和为1；\\
\frac{1}{p(x)}代表不确定性，log是表示复杂度。 \\
其中的 b 代表底数，比如 2, 10, e 等等，常用的就是 2，这符合计算机中的二进制。 \\
熵的最大值为 \log{N}，即这是一个均匀分布的系统，每个事件发生的概率都是 \frac{1}{N}，正如上面的掷骰子。\\
熵的最小值为 0
$$
![image-20240820223522171](D:\dev\php\magook\trunk\server\md\img\image-20240820223522171.png)

![image-20240820225917694](D:\dev\php\magook\trunk\server\md\img\image-20240820225917694.png)

![image-20240820225939852](D:\dev\php\magook\trunk\server\md\img\image-20240820225939852.png)

对于上面的这两个分布P(x)，我们来计算它们的熵。

```python
import numpy as np


def H(px):
    '''
    计算信息熵
    '''
    sum = 0
    for p in px:
        sum += p*np.log2(1/p)

    return sum


def generate():
    '''
    随机生成分布
    '''
    lt = np.random.randn(10)*100
    lt = abs(lt)
    sum = 0
    ltint = []
    p = []
    for i in lt:
        ins = int(i)
        ltint.append(ins)
        sum += ins

    for i in ltint:
        k = round(i/sum, 2)
        if k == 0:
            generate()

        p.append(k)

    psum = 0
    for i in p:
        psum += i

    return p


if __name__ == "__main__":
    r1 = H([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])  # 均匀分布

    p = generate()
    print(p)

    r2 = H(p)

    print(r1)
    print(r2)
    print(np.log2(10))  # 熵的最大值为 logN

```

输出结果

```
[0.04, 0.27, 0.01, 0.07, 0.04, 0.14, 0.23, 0.02, 0.07, 0.11]
3.321928094887362
2.833020458071243
3.321928094887362
```



熵在深度学习中最重要的应用就是，熵能够定量地度量一个模型。如果两个模型属于同一概率分布（例如高斯分布），就没有必要引入熵的概念，直接比较两个模型即可，也就是直接用信息量来做。但是如果两个模型一个是高斯分布，一个是[泊松分布](https://www.zhihu.com/search?q=泊松分布&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2517655344})，就没办法直接比较，必须引入熵的概念。在深度学习的实践中，很明显大部分模型都是大量分布的组合，有时候甚至根本不知道是什么分布，这个时候由于熵可以度量整个系统，其意义就显现出来了。

**交叉熵**是用来评估当前训练得到的**概率分布**与真实分布的差异情况。 它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，不确定性越小，两个概率分布就越接近。

交叉熵：信息论和深度学习的桥梁。交叉熵越小，损失就越小，两个模型越接近。且交叉熵定义为
$$
交叉熵：H(P,Q)=-\sum_{i=1}^{n}P(x_{i})\log{Q(x_{i})} \\
交叉熵：H(Y,\hat{Y})=-\sum_{i=1}^{n}Y_{i}·\log{\hat{Y}_{i}} \\
相对熵（KL散度）：D(P||Q)=H(P,Q)-H(P)=-\sum_{i=1}^{n}P(x_{i})\log{\frac{Q(x_{i})}{P(x_{i})}} 
$$
在公式上，交叉熵和极大似然估计是完全一样的。



为什么使用交叉熵代替均方误差MSE：1、交叉熵权重更新更快。



**为什么极大似然估计与交叉熵的公式一样**





**什么是模型的收敛**

一个模型，应该是在输入变量不变的时候，输出也应该是确定的。

在说模型收敛的时候，一般指的是训练、验证损失曲线没有大的波动，而且随着训练轮数不断增加，波动依然可以在一定容忍范围内。

个人理解，收敛的意义是系统稳定，就是模型的某一个权重参数发生小的改变的时候，模型输出结果不会发生强烈变化，导致系统崩溃（也就是所谓的发散）。

反过来说，发散就是在模型参数发生微小改变的时候，模型输出变化，导致损失剧烈变化。此时的模型就不是训练充分的模型。

实际上，我们目前所说的模型收敛指的是损失函数接近稳定状态，或者更确切说应该指的是损失函数的收敛。

loss值高但是稳定不变当然可以叫作收敛了，因为loss高可能只是模型不够好造成的，但还是收敛了的。但是模型收不收敛的本质不是看loss，而是看模型参数是否有大的变化。loss稳定不变只是参数趋于稳定的一种体现，基本等价于模型收敛。



**什么是神经元，感知机，激活函数Activation Function**

很形象的一个视频解释神经网络NN（类比旅客登机的过程）：https://www.bilibili.com/video/BV1YY411a7F7/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0

关于激活函数的生动解释：https://zhuanlan.zhihu.com/p/25279356

神经网络为什么可以拟合任意函数：https://mp.weixin.qq.com/s/I9w7RlBkedwqiYAr3UB6xA

卷积神经网络CNN就是在神经网络前面加上了一些卷积层（`输入层-->卷积层——>池化层-->全连接层-->输出层`）。

激活函数的作用就是将上一层的结果做一个转换得到结果Y，使其看上去特征明显，很容易就能判断和分类，比如Y的结果只有0和1。



关于Relu函数：https://blog.csdn.net/cherrylvlei/article/details/53149381

将上一个神经元的输出作为 x ，输入到下一个神经元的激活函数中去，依次往前。

有一条直线，上面有一些点（激活函数），通过拉扯这些点可以将直线变成任意曲线（俗称加入非线性因素），扩展到多维空间就是任意超曲面，目的就是为了分类。
$$
y=f(\sum_{i=1}^{n}w_{i}x_{i}+b)
$$
将上游函数的结果y当做激活函数的x代入之，得到一些奇特的效果。比如 Sigmoid 激活函数，他取值(0, 1)，两端无限趋近于0或1，相相当于忽略掉了两端的波动，只关注中间这块的信息，起到过滤的作用。

![image-20240815115059107](D:\dev\php\magook\trunk\server\md\img\image-20240815115059107.png)

在神经元中，激活函数是很重要的一部分，为了增强网络的表示能力和学习能力，神经网络的激活函数都是非线性的，通常具有以下几点性质：

- 连续并可导（允许少数点上不可导），可导的激活函数可以直接利用数值优化的方法来学习网络参数；
- 激活函数及其导数要尽可能简单一些，太复杂不利于提高网络计算率；
- 激活函数的导函数值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。





**循环神经网络RNN**

https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0

有记忆能力，可以理解一定的上下文，但是记忆力也就十步之内。

可以理解为一个神经网络NN，在不同的时刻`t`上有不同的NN，它们有不同的参数值，因此存储着不同的信息，但是`t`时刻的神经元可以从`t-1`时刻的神经元获取信息，这就是上下文。所谓的循环指的是时间上可以一层一层的从过去获得信息。

![image-20240814221559991](D:\dev\php\magook\trunk\server\md\img\image-20240814221559991.png)

![image-20240814221525926](D:\dev\php\magook\trunk\server\md\img\image-20240814221525926.png)



CNN主要用于图像识别。

RNN可以理解上下文，可以用于NLP，语音，视频处理等，但是现在被自注意力模型transformer超越，但是RNN在小数据集，低算力的情况下依然可以非常有效。



**LSTM-RNN**

https://www.bilibili.com/video/BV1Z34y1k7mc/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0

为了解决RNN的记忆短的问题，人们发明了`长短期记忆网络LSTM（Long Short-term Memoery）`。

LSTM的秘诀就是增加了一个日记本存储长期记忆，并且有增加和删除日记的操作，这称为长期记忆。而短期记忆还是RNN来提供。

![image-20240814223903361](D:\dev\php\magook\trunk\server\md\img\image-20240814223903361.png)

粉色的三个C层就是长期记忆，也就是日记本，他与RNN中的隐藏层连接，互相通信，

![image-20240814224321273](D:\dev\php\magook\trunk\server\md\img\image-20240814224321273.png)



sigmoiid 函数取值(0, 1)，因此会抹掉一些不重要的信息，起到擦除作用，相当于选择性遗忘了部分记忆，相当于一个阀门，过滤重要特征，忽略无关信息。

tanh 函数取值(-1, 1)，相对于把这两天发生的事情进行梳理和归纳，也就是更新日记。



![image-20240814224608174](D:\dev\php\magook\trunk\server\md\img\image-20240814224608174.png)

Ct就是得到的新日记。



**反向传播BackPropagation**

误差从输出向后倒查分解，以便根据梯度更新神经元连接的权重。





**自注意力机制attention**

https://www.bilibili.com/video/BV1xS4y1k7tn/?spm_id_from=333.788&vd_source=0c75dc193ee55511d0515b3a8c375bd0

attention就是权重。以输入为中心（称为自注意力self-attention），在全部节点中查找与他关联最紧密的信息，即权重最高的一些信息，而这个权重是由训练的时候设置的。于是模型就像理解了你的上下文一样。

transformer算法

BERT模型

GPT模型

attention机制的三大优点：参数更少，速度更快，效果更好。



**transformer算法**

https://www.bilibili.com/video/BV1MY41137AK/?spm_id_from=pageDriver&vd_source=0c75dc193ee55511d0515b3a8c375bd0

就是基于自注意力机制实现的。在机器翻译领域表现巨好，近年来横扫NLP领域。





**什么是鲁棒性**

鲁棒性，英文是[Robustness](https://www.zhihu.com/search?q=Robustness&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2927152951})，也可翻译为稳健性，指模型在陌生环境或噪声干扰下依旧能够完成预期任务的能力。



**优化optimization与优化器optimizer**

在一个函数 J 中有一些参数，记为 θ ，比如 w=ax+by+cz，那么θ 泛指(a,b,c)，现在有一些样本值，我们需要计算出使得 w 取值最小时的(a,b,c)参数。

![image-20240811170454899](D:\dev\php\magook\trunk\server\md\img\image-20240811170454899.png)

【香农的信息论究竟牛在哪里？】https://www.zhihu.com/question/27068465/answer/2517655344

![image-20240811170633803](D:\dev\php\magook\trunk\server\md\img\image-20240811170633803.png)



**随机梯度下降算法SGD**

**1、什么是梯度 gradient**

https://www.zhihu.com/question/29151564/answer/1827501472

可以理解为斜率，比如，垂直线的梯度为九十，水平的梯度为零。梯度包含大小和方向。梯度用一个倒三角形▽表示。

梯度的值就是求导，对于一元多次方程（曲线），梯度的值是导数；对于多元多次方程（曲面），梯度的值就是求偏导，然后再将具体的点坐标代入公式得到结果；于是一元多次方程在某点处的梯度就是一个值（斜率），而多元多次方程在某点处的梯度是一个向量，因为每一个维度都有一个梯度值。

假如有方程(ax+by+c+d)，其参数为(a, b, c, d)，我们将(x, y)看成常量（因为有样本数据），于是方程就变成了关于(a, b, c, d)的函数，要计算此函数的最小值，我们来求梯度
$$
\nabla a=\frac{\partial f}{\partial a} \\
\nabla b=\frac{\partial f}{\partial b} \\
\nabla c=\frac{\partial f}{\partial c} \\
\nabla d=\frac{\partial f}{\partial d} \\
\nabla f= [\nabla a, \nabla b, \nabla c, \nabla d]
$$
首先给参数(a, b, c, d)一个初始值，设置一个步长（学习率） η，然后根据梯度可以更新参数
$$
a=a-\eta · \nabla a \\
b=b-\eta · \nabla b \\
c=c-\eta · \nabla c \\
b=d-\eta · \nabla d \\
$$
接下来设置目标结果，也就是收敛条件，常见的收敛条件包括：

- 损失函数值的变化量小于预设的阈值。
- 梯度的范数小于预设的阈值。
- 达到最大迭代次数。

梯度下降法优点：

1. **通用性强**：适用于各种类型的优化问题。
2. **计算效率高**：每次迭代只需计算一次梯度，适合大规模数据集。
3. **实现简单**：算法原理简单，易于实现。
4. **可扩展性**：可以结合多种技术（如动量、学习率衰减等）改进性能。

缺点：

1. **容易陷入局部最优**：对于非凸函数，可能会停留在局部最优点而非全局最优点。
2. **收敛速度慢**：对于复杂的损失函数，收敛速度可能较慢。
3. **依赖初始值**：初始参数选择对结果影响较大。
4. **学习率选择困难**：学习率太大或太小都会影响收敛效果。



梯度下降法的实例：

https://mp.weixin.qq.com/s/gizUmdNcvSyUFhSRWVjXtA

https://mp.weixin.qq.com/s/IAkVWrWGMZCGAwxIhRysLw



如果损失函数是均方误差MSE，那么可以推导出梯度的计算公式
$$
假设：y=ax+b \\
损失函数：f(y,\hat{y}) = \sum(y-\hat{y})^2 \\
那么：\nabla a=\frac{\partial f}{\partial a}= \frac{\partial (\sum(y-\hat{y})^2)}{\partial a} \\
=2·(y-\hat{y})·\frac{\partial (\sum(y-\hat{y}))}{\partial a} \\
=2·(y-\hat{y})·\frac{\partial y}{\partial a} \\
=2·(y-\hat{y})·x \\
然后：\nabla b=2·(y-\hat{y}) \\
所以，综合来看，\theta_{n}=\theta_{n-1}-\eta·(y-\hat{y})
$$




**2、梯度下降法**

https://www.zhihu.com/question/305638940/answer/1639782992

https://www.zhihu.com/question/264189719/answer/2882977899

梯度下降法是神经网络模型训练中最为常见的优化器，尽管梯度下降法很少直接用于深度学习，但理解它是理解随机梯度下降法和小批量随机梯度下降法的基础，他是用来计算函数最小值的。有的函数通过手算可以算出其最小值，但是有的函数太过于复杂，这时可以使用计算机的能力来试探出最小值。

梯度下降法最好的例子就是大自然中的泉水下山的例子：1、水受重力影响，会在当前位置，沿着**最陡峭**的方向流动，有时会形成瀑布（**梯度的反方向为函数值下降最快的方向**）；2、水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）；3、遇到坑洼地区，有可能形成湖泊，而终止下山过程（无法得到[全局最优解](https://www.zhihu.com/search?q=全局最优解&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2882977899})，只能得到局部最优解）。

[凸函数](https://www.zhihu.com/search?q=凸函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A1639782992})图像看上去就像一个山谷，如果运用梯度下降法的话，就可以通过一步步的滚动最终来到谷底，也就是找到了函数的最小值。

![image-20240809143603609](D:\dev\php\magook\trunk\server\md\img\image-20240809143603609.png)

梯度为正，说明是向上的，反方向就是向下的，然后乘以步长就是移动的距离，直到梯度为零。

二维平面的梯度下降

![image-20240809174519960](D:\dev\php\magook\trunk\server\md\img\image-20240809174519960.png)

步长过小则计算的点就多，梯度过大则会失效，甚至越过谷底不断爬升。

![image-20240809145325919](D:\dev\php\magook\trunk\server\md\img\image-20240809145325919.png)

三维平面的梯度下降

![image-20240809174436469](D:\dev\php\magook\trunk\server\md\img\image-20240809174436469.png)

梯度下降法的好处就是，这个算法可以知道哪个参数对产生这个差值贡献的多，哪个贡献的少，贡献多的多调整，贡献少的少调整。

上面我们已经充分了解了梯度下降法，剩下的还要考虑该如何使用这些样本，主要是梯度的计算方式不同。分为：

- 批量梯度下降法BGD：把所有样本都计算一遍，得到N个梯度值，取平均值，然后去更新 θ。
- 随机梯度下降法SGD：随机取一个样本来计算梯度作为此次迭代的梯度，然后去更新 θ。
- 小批量梯度下降法MBGD：取一个小批量的样本来计算梯度，然后去更新 θ。推荐。

当训练样本数很大时，批量梯度下降的每次更新都会是计算量很大的操作，而随机梯度下降可以利用单个训练样本立即更新，因此随机梯度下降 通常是一个更快的方法。但随机梯度下降也有一个缺点，那就是θ可能不会收敛，而是在最小值附近振荡，但在实际中也都会得到一个足够好的近似。所以实际情况下，我们一般不用固定的学习率，而是让它随着算法的运行逐渐减小到零，也就是在接近“山底”的时候慢慢减小下降的“步幅”，换成用“小碎步”走，这样它就更容易收敛于全局最小值而不是围绕它振荡了。

**3、随机梯度下降法SGD**

他是梯度下降算法的扩展。

每一次随机选择一个样本进行梯度计算。行进路线就像醉汉，比较波折。好处是提升了计算速度（每次只需要计算一个样本点），但是却牺牲了精准度，虽然大方向是对的。

https://www.zhihu.com/question/264189719/answer/2694337482

![image-20240811162522099](D:\dev\php\magook\trunk\server\md\img\image-20240811162522099.png)



4、小批量梯度下降法MBGD

每次计算的时候，选取当前点附近的几个点进行计算，找到梯度下降最多的方向，称为小批量。

![image-20240811162732764](D:\dev\php\magook\trunk\server\md\img\image-20240811162732764.png)

随机梯度下降法由于样本的随机性会带来很多噪声（就是比较曲折不稳定），我们可以选取一定数目的样本组成一个小批量样本，然后用这个小批量更新梯度，这样不仅可以减少[计算成本](https://www.zhihu.com/search?q=计算成本&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A932262940})，还可以提高算法稳定性。



![image-20240811171211774](D:\dev\php\magook\trunk\server\md\img\image-20240811171211774.png)

![image-20240811171619880](D:\dev\php\magook\trunk\server\md\img\image-20240811171619880.png)

![image-20240811171857437](D:\dev\php\magook\trunk\server\md\img\image-20240811171857437.png)

![image-20240811172037692](D:\dev\php\magook\trunk\server\md\img\image-20240811172037692.png)

![image-20240811172204575](D:\dev\php\magook\trunk\server\md\img\image-20240811172204575.png)





梯度下降法对学习率的值非常敏感，太大或太小都不行。另外，有可能找到的是局部最低点，而不是全局最低点。为了解决这些问题，就有了:

动态调整学习率的AdaGrad

优化动态学习率RMSProp

无需设置学利率AdaDelta

融合了AdaGrad与RMSProp的算法Adam

模拟动量Momentum







页码：118/146
