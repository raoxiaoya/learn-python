深度学习 Deep Learning 笔记（4）-数值计算



#### 4.1、上溢和下溢

计算机中由于难以精确的表示浮点数，通常会采用近似表示，或者四舍五入，这会导致一定的舍入误差，但是不加以干预，这些误差的累计会导致算法的失效。

下溢：将接近零的数值四舍五入为零。在数学中无限接近零的参数和零值完全是不同的东西。

上溢：数学中的无穷大，在计算机中是不存在的，因为计算机能存储的数的上限是64位或者32位，而不是无限大。



#### 4.2、病态条件



#### 4.3、基于梯度的优化方法

深度学习算法的优化，指的是改变 x 使 f(x) 最小化或最大化，我们通常以最小化 *f*(x) 指代大多数最优化问题。最大化可经由最小化算法最小化 *−f*(x) 来实现。

我们把要最小化或最大化的函数称为 **目标函数**（objective function）或 准则（criterion）。当我们对其进行最小化时，我们也把它称为 **代价函数**（cost function）、**损失函数**（loss function）或 **误差函数**（error function）。虽然有些机器学习著作赋予这些名称特殊的意义，但在这本书中我们交替使用这些术语。

**损失函数**是用于评估这个模型所作出的预测离真实值（**Ground Truth**）之间的偏离程度。 通常，我们都会最小化目标函数，最常用的算法便是**梯度下降法**（**Gradient Descent**）。

可以把损失函数看成一个评判者，当模型在训练时，将模型预测出的结果交给损失函数来评判，评判的标准是人为设置的值（或者人为设置的标签），如果差距较大，就会给与反馈，模型就会自动调整参数，如此反复下去，直到误差最小化。

**损失函数**又分为分类损失，回归损失，排名损失。

损失函数常用的是**最小二乘法，极大似然估计法，交叉熵**。



**导数与优化之间的联系**：以 `y=f(x)` 为例，求导之后为`f'(x)`，它表示 x 变化一个微小的值 y 的变化。也就是在 x 处的斜率，如果导数大于零，那么 x 增加会导致 y 增加，x 减小会导致 y 减小；如果导数小于零则相反。通过判断导数的正负就可以知道该对 x 进行增加还是减小，最终达到减小 y 的目的。基于此原理的损失函数为**梯度下降损失函数（gradient descent）**。

![image-20240703115617231](D:\dev\php\magook\trunk\server\md\img\image-20240703115617231.png)

当导数为零时，无法提供往哪个方向移动的信息，此时的 x 称为临界点或者驻点。而驻点又分为三种情况，如下图

![image-20240703140452396](D:\dev\php\magook\trunk\server\md\img\image-20240703140452396.png)



临界点

驻点

局部极小点

局部极大点

全局极小点

全局极大点

鞍点

偏导数

梯度

方向导数

最速下降法

梯度下降

爬山算法

梯度之上：**Jacobian** 和 **Hessian** 矩阵

二阶导数

牛顿法

一阶优化算法

二阶优化算法

凸优化



随机梯度下降 SGD （Stochastic Gradient Descent）：每次喂一个数据进去，求一个数据上的梯度，很震荡。

mini-batch 梯度下降：每次喂一小批数据进去，较震荡。

batch 梯度下降：每次喂全部数据进去，求全局梯度，不震荡。



![image-20240703224429571](D:\dev\php\magook\trunk\server\md\img\image-20240703224429571.png)



![image-20240703224624885](D:\dev\php\magook\trunk\server\md\img\image-20240703224624885.png)



#### 4.4、约束优化

KKT（**Karush**–**Kuhn**–**Tucker**）

广义 **Lagrangian**（generalized Lagrangian）

等式约束（equality constraint）

不等式约束（inequality constraint）



#### 4.5、实例：最小二乘法与回归直线

在一个二维直角坐标系中有一系列的点，他们大致符合 y=ax+b，现在要求出参数 a 和 b 的值。

最小二乘法的思想是这样的：点 (x0, y0)处，将 x0 带入到上面的直线可以得到 y，计算 (y-y0)^2，然后求和，要求和值最小，即可求出a和b的值。
$$
Q(a, b) = \sum_{i=1}^{n}(ax_{i}+b-y_{i})^2
$$
这条直线称为回归直线。

分别对 a 和 b 求偏导，令导数等于0
$$
\frac{\partial Q}{\partial a} = \sum_{i=1}^{n}2x_{i}(ax_{i}+b-y_{i}) = 0 \\
\frac{\partial Q}{\partial b} = \sum_{i=1}^{n}2(ax_{i}+b-y_{i}) = 0 
$$
为什么要令导数等于0呢？那是因为，在求导之前，Q(a, b)分别是关于a和b的二次方程，而二次方程是个开口向上的抛物线，最低点的导数就是零。

整理后得到：
$$
a = \frac{\sum_{i=1}^{n}(x_{i}-x^{'})(y_{i}-y^{'})}{\sum_{i=1}^{n}(x_{i}-x^{'})^2} = \frac{\sum_{i=1}^{n}x_{i}y_{i}-nx^{'}y^{'}}{\sum_{i=1}^{n}x_{i}^2-nx^{'2}} \\
\\ 
b = y^{'}-ax^{'} \\
其中 x^{'}, y^{'} 分别为这些点的 x 和 y 的平均值
$$
![image-20240703223554386](D:\dev\php\magook\trunk\server\md\img\image-20240703223554386.png)